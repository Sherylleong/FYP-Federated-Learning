{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49993f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r'models\\fl_fedavg\\effnet-b0\\ff_effnet0_1fc_fl.pth'\n",
    "video_path = r'000.mp4'\n",
    "#image_path = r'test_original.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a9e7519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from facenet_pytorch import MTCNN\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "IMG_SIZE = 224\n",
    "\n",
    "\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std = (0.229, 0.224, 0.225)\n",
    "\n",
    "class ImageTransform:\n",
    "    def __init__(self, size, mean, std, train=False):\n",
    "        self.data_transform = transforms.Compose([\n",
    "            transforms.Resize((size, size), interpolation=Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img):\n",
    "        return self.data_transform(img)\n",
    "\n",
    "\n",
    "\n",
    "transformer = ImageTransform(IMG_SIZE, mean, std, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c432b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n",
      "c:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(state_dict_path)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Coordinate 'right' is less than 'left'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m pil_img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame_rgb)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# ====== 3. Detect faces ======\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mmtcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpil_img\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m faces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] No face detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:265\u001b[0m, in \u001b[0;36mMTCNN.forward\u001b[1;34m(self, img, save_path, return_prob)\u001b[0m\n\u001b[0;32m    261\u001b[0m     batch_boxes, batch_probs, batch_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_boxes(\n\u001b[0;32m    262\u001b[0m         batch_boxes, batch_probs, batch_points, img, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselection_method\n\u001b[0;32m    263\u001b[0m     )\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# Extract faces\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_prob:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m faces, batch_probs\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:490\u001b[0m, in \u001b[0;36mMTCNN.extract\u001b[1;34m(self, img, batch_boxes, save_path)\u001b[0m\n\u001b[0;32m    487\u001b[0m     save_name, ext \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(path_im)\n\u001b[0;32m    488\u001b[0m     face_path \u001b[38;5;241m=\u001b[39m save_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m ext\n\u001b[1;32m--> 490\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[43mextract_face\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmargin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process:\n\u001b[0;32m    492\u001b[0m     face \u001b[38;5;241m=\u001b[39m fixed_image_standardization(face)\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:370\u001b[0m, in \u001b[0;36mextract_face\u001b[1;34m(img, box, image_size, margin, save_path)\u001b[0m\n\u001b[0;32m    362\u001b[0m raw_image_size \u001b[38;5;241m=\u001b[39m get_size(img)\n\u001b[0;32m    363\u001b[0m box \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m margin[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmax\u001b[39m(box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m margin[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)),\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m margin[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, raw_image_size[\u001b[38;5;241m0\u001b[39m])),\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mmin\u001b[39m(box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m margin[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, raw_image_size[\u001b[38;5;241m1\u001b[39m])),\n\u001b[0;32m    368\u001b[0m ]\n\u001b[1;32m--> 370\u001b[0m face \u001b[38;5;241m=\u001b[39m \u001b[43mcrop_resize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    373\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(save_path) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:324\u001b[0m, in \u001b[0;36mcrop_resize\u001b[1;34m(img, box, image_size)\u001b[0m\n\u001b[0;32m    319\u001b[0m     out \u001b[38;5;241m=\u001b[39m imresample(\n\u001b[0;32m    320\u001b[0m         img\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat(),\n\u001b[0;32m    321\u001b[0m         (image_size, image_size)\n\u001b[0;32m    322\u001b[0m     )\u001b[38;5;241m.\u001b[39mbyte()\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mresize((image_size, image_size), Image\u001b[38;5;241m.\u001b[39mBILINEAR)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Sheryl\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\PIL\\Image.py:1214\u001b[0m, in \u001b[0;36mImage.crop\u001b[1;34m(self, box)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m<\u001b[39m box[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m   1213\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordinate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is less than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m<\u001b[39m box[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m   1216\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordinate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is less than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Coordinate 'right' is less than 'left'"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN(margin=300, select_largest=False, factor=0.5, device=device, post_process=False) # post_process=False if want human readable image\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "frame_interval = 32\n",
    "frame_idx = 0\n",
    "predictions = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_idx % frame_interval == 0:\n",
    "        # Convert frame to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(frame_rgb)\n",
    "\n",
    "        # ====== 3. Detect faces ======\n",
    "        faces = mtcnn(pil_img)\n",
    "\n",
    "        if faces is None:\n",
    "            print(f\"[Frame {frame_idx}] No face detected.\")\n",
    "        else:\n",
    "            # Ensure list of faces\n",
    "            if isinstance(faces, torch.Tensor):\n",
    "                faces = [faces]\n",
    "\n",
    "            for i, face in enumerate(faces):\n",
    "                face_tensor = transform(face).unsqueeze(0).to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    output = torch.sigmoid(model(face_tensor))\n",
    "                    prob = output.item()\n",
    "                    predicted_class = int(prob > 0.5)\n",
    "\n",
    "                predictions.append({\n",
    "                    'frame': frame_idx,\n",
    "                    'face_idx': i,\n",
    "                    'prob': prob,\n",
    "                    'predicted_class': predicted_class\n",
    "                })\n",
    "\n",
    "                print(f\"[Frame {frame_idx} - Face {i+1}] Class: {predicted_class}, Prob: {prob:.4f}\")\n",
    "\n",
    "    frame_idx += 1\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ebc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
